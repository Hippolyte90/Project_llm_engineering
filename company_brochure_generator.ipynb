{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84df3d27-9806-406f-907e-defad2f77ef5",
   "metadata": {},
   "source": [
    "## Building a company brochure generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb81e44-b9e9-4b08-a089-66c4481a89aa",
   "metadata": {},
   "source": [
    "### We will use Gradio framework to generate a company brochure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00999d82-6382-4e07-8fed-dd276f3c8703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import google.generativeai\n",
    "from langdetect import detect\n",
    "#import anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7116b181-2d0b-41c7-b027-b9ed6394ae2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr # oh yeah!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4884f8ac-6c8b-4201-8172-b18d1d41e071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "GROQ API Key exists and begins gsk_fo6e\n",
      "Google API Key exists and begins AIzaSyAx\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables in a file called .env\n",
    "# Print the key prefixes to help with any debugging\n",
    "# We can use claude if you have the API KEY. For that, uncomment the line concerning anthropic (claude) \n",
    "\n",
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "#anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "\n",
    "if groq_api_key:\n",
    "    print(f\"GROQ API Key exists and begins {groq_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"GROQ API Key not set\")\n",
    "    \n",
    "#if anthropic_api_key:\n",
    " #   print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "#else:\n",
    " #   print(\"Anthropic API Key not set\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1050d68-c339-44af-b20f-eaef852d1a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to OpenAI, Anthropic and Google\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "#claude = anthropic.Anthropic()\n",
    "\n",
    "google.generativeai.configure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d78754bd-0de7-4dff-8123-6f1c42ad6600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A generic system message - no more snarky adversarial AIs!\n",
    "\n",
    "system_message = \"You are a helpful assistant\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9d31e69-729f-4dbf-a037-2b0f5e9f8ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use GPT-4o-mini for wraping \n",
    "\n",
    "def message_gpt(prompt):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "      ]\n",
    "    completion = openai.chat.completions.create(\n",
    "        model='gpt-4o-mini',\n",
    "        messages=messages,\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f976336-b794-4879-ae0e-ea72895037c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"LLM (Large Language Model) engineering is an exciting and rapidly evolving field within artificial intelligence. It involves the development, fine-tuning, and application of large language models such as GPT-3 and beyond, to solve various problems and enhance human-computer interactions. Here are some key aspects of LLM engineering:\\n\\n1. **Model Development**: It includes creating more sophisticated models with better architecture, training techniques, and methods to handle biases and inefficiencies present in data.\\n\\n2. **Fine-tuning and Customization**: LLMs can be fine-tuned on specific datasets to better serve particular applications, allowing them to perform well in niche domains without losing their general understanding.\\n\\n3. **Prompt Engineering**: Generating effective prompts to elicit desired responses from language models is crucial for maximizing their utility. This requires an understanding of both the model's capabilities and the nuances of language.\\n\\n4. **Ethics and Bias**: LLMs can inadvertently perpetuate societal biases. Addressing ethical considerations and implementing strategies to mitigate these risks is a significant part of LLM engineering.\\n\\n5. **Applications**: LLMs have a broad range of applications, from customer service chatbots and content generation to more complex uses like coding aids and knowledge synthesis.\\n\\n6. **Interdisciplinary Nature**: LLM engineering combines expertise from various fields, including computer science, linguistics, psychology, and domain-specific knowledge, which requires collaboration across disciplines.\\n\\n7. **Evaluation and Metrics**: Defining proper metrics and evaluation methods to assess the performance of LLMs is essential to ensure they meet the desired standards and improve over time.\\n\\n8. **Scalability**: As demands on LLMs grow, scaling models and infrastructure to handle increased loads effectively while maintaining performance and response times poses technical challenges.\\n\\nOverall, LLM engineering is a dynamic area that holds great potential for innovation and practical applications, while also requiring careful consideration of its implications and ethical responsibilities. As the technology advances, ongoing research will be important to harness its benefits while addressing its challenges.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A test of this function\n",
    "\n",
    "message_gpt(\"What do you think about LLMs Engineering?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa65e16-aa4d-4986-867b-6a48d35ae1ee",
   "metadata": {},
   "source": [
    "## User Interface using simply function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c08b987-fe1f-4d34-a299-f347c93a3b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _up(text):\n",
    "    print(f\"Shout has been called with input {text}\")\n",
    "    return text.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8964cfa-749c-4cff-bde5-c8433a3f45e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shout has been called with input hello\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'HELLO'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_up(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70c48b53-fdbd-4b15-aaca-bb81ee7ecbf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7874\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7874/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shout has been called with input hello!\n"
     ]
    }
   ],
   "source": [
    "# Simply Gradio user interface\n",
    "gr.Interface(fn=_up, inputs=\"textbox\", outputs=\"textbox\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d4c161a-0866-4b64-a516-a989b867a050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7875\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7875/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shout has been called with input hello!\n"
     ]
    }
   ],
   "source": [
    "## Interface without Flag\n",
    "gr.Interface(fn=_up, inputs=\"textbox\", outputs=\"textbox\", flagging_mode=\"never\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34d2737e-a67e-4cd4-b665-9cdb61218973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7877\n",
      "\n",
      "Could not create share link. Please check your internet connection or our status page: https://status.gradio.app.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7877/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shout has been called with input hello!\n"
     ]
    }
   ],
   "source": [
    "## To get public link, we 'll add share=True in launch()\n",
    "gr.Interface(fn=_up, inputs=\"textbox\", outputs=\"textbox\", flagging_mode=\"never\").launch(share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80fe2967-b2d7-4fee-a82c-4538851cfe72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7878\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7878/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shout has been called with input hello\n",
      "Shout has been called with input Hello!\n"
     ]
    }
   ],
   "source": [
    "# Adding inbrowser=True opens up a new browser window automatically\n",
    "\n",
    "gr.Interface(fn=_up, inputs=\"textbox\", outputs=\"textbox\", flagging_mode=\"never\").launch(inbrowser=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1bb34a24-4c25-4a9b-b96a-9732008f773a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7879\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7879/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shout has been called with input Hello\n"
     ]
    }
   ],
   "source": [
    "# Define this variable and then pass js=force_dark_mode when creating the Interface\n",
    "\n",
    "force_dark_mode = \"\"\"\n",
    "function refresh() {\n",
    "    const url = new URL(window.location);\n",
    "    if (url.searchParams.get('__theme') !== 'dark') {\n",
    "        url.searchParams.set('__theme', 'dark');\n",
    "        window.location.href = url.href;\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "gr.Interface(fn=_up, inputs=\"textbox\", outputs=\"textbox\", flagging_mode=\"never\", js=force_dark_mode).launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "953d9faf-9a58-4891-8b28-29205e56d353",
   "metadata": {},
   "outputs": [],
   "source": [
    "##  GPT model\n",
    "def stream_gpt(prompt):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "      ]\n",
    "    stream = openai.chat.completions.create(\n",
    "        model='gpt-4o-mini',\n",
    "        messages=messages,\n",
    "        stream=True\n",
    "    )\n",
    "    result = \"\"\n",
    "    for chunk in stream:\n",
    "        result += chunk.choices[0].delta.content or \"\"\n",
    "        yield result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "36dce695-cb98-4fe2-87c4-405d54767cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gemini model\n",
    "def stream_gemini(prompt):\n",
    "    gemini = google.generativeai.GenerativeModel(\n",
    "        model_name='gemini-2.0-flash',\n",
    "        system_instruction=system_message\n",
    "    )\n",
    "    result = gemini.generate_content(prompt, stream=True)\n",
    "    response = \"\"\n",
    "    for chuck in result:\n",
    "        text = chuck.text\n",
    "        response += text or \"\"\n",
    "        yield response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0cfb7067-be9c-497f-819a-31a322df6807",
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Groq model\n",
    "def stream_groq(prompt):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "      ]\n",
    "    groq = OpenAI(api_key = groq_api_key, base_url=\"https://api.groq.com/openai/v1\")\n",
    "    stream = groq.chat.completions.create(\n",
    "        model=\"llama-3.3-70b-versatile\",\n",
    "        messages=messages,\n",
    "        stream=True\n",
    "    )\n",
    "    result = \"\"\n",
    "    for chunk in stream:\n",
    "        result += chunk.choices[0].delta.content or \"\"\n",
    "        yield result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4b70b8b4-6d7a-410b-b779-3a01dd3b7316",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object stream_groq at 0x0000027A1E9E2040>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stream_groq(\"Hello!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99adc3ee-71b1-4921-8c97-c9fb3b1662c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If ollama is not set up, install and do it.\n",
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c443bfc8-5424-4ee1-9bd4-29eb70fab98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ollama model\n",
    "def stream_ollama(prompt):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "      ]\n",
    "    stream = ollama.chat(\n",
    "        model=\"llama3.2\",\n",
    "        messages=messages,\n",
    "        stream=True\n",
    "    )\n",
    "    result = \"\"\n",
    "    for chunk in stream:\n",
    "        result += chunk['message']['content'] or \"\"\n",
    "        yield result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dcbd139f-af0a-4667-8c04-6a984d77154f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object stream_ollama at 0x0000027A1E9E2840>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stream_ollama(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "732d37dc-afa8-4b64-9530-6a20f1c32f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_model(prompt, model):\n",
    "    if model==\"GPT\":\n",
    "        result = stream_gpt(prompt)\n",
    "    elif model==\"Gemini\":\n",
    "        result = stream_gemini(prompt)\n",
    "    elif model == \"Groq\":\n",
    "        result = stream_groq(prompt)\n",
    "    elif model == \"Ollama\":\n",
    "        result = stream_ollama(prompt)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown model\")\n",
    "    yield from result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329d5081-7279-44fc-b340-36dfcc7bed4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Try with question \"What do you think about AI solution which is provide to the company?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8ebafd06-c5d6-4449-8de4-e0276d4f4603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7888\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7888/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "view = gr.Interface(\n",
    "    fn=stream_model,\n",
    "    inputs=[gr.Textbox(label=\"Your message:\", lines = 8), gr.Dropdown([\"GPT\", \"Gemini\", \"Groq\", \"Ollama\"], label=\"Select model\", value=\"GPT\")],\n",
    "    outputs=[gr.Markdown(label=\"Response:\")],\n",
    "    flagging_mode=\"never\",\n",
    "    js=force_dark_mode\n",
    ")\n",
    "view.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63eac674-9540-4cf3-951b-c4465831d879",
   "metadata": {},
   "source": [
    "## Brochure generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "795fa6b3-a389-44da-a21d-9ee0a5d89809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A class to represent a Webpage\n",
    "\n",
    "class Website:\n",
    "    url: str\n",
    "    title: str\n",
    "    text: str\n",
    "\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        response = requests.get(url)\n",
    "        self.body = response.content\n",
    "        soup = BeautifulSoup(self.body, 'html.parser')\n",
    "        self.title = soup.title.string if soup.title else \"No title found\"\n",
    "        for irrelevant in soup.body([\"script\", \"style\", \"img\", \"input\"]):\n",
    "            irrelevant.decompose()\n",
    "        self.text = soup.body.get_text(separator=\"\\n\", strip=True)\n",
    "\n",
    "    def get_contents(self):\n",
    "        return f\"Webpage Title:\\n{self.title}\\nWebpage Contents:\\n{self.text}\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5655094a-d596-41df-919c-9ce9668aef78",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model role\n",
    "system_message1 = \"You are an assistant that analyzes the contents of a company website landing page \\\n",
    "and creates a short brochure about the company for prospective customers, investors and recruits. Respond in markdown.\\\n",
    "Use the same language like what is use on the landing page.\"\n",
    "\n",
    "system_message2 = system_message1 + \"Use a sympathetic and empathetic tone\"\n",
    "\n",
    "system_message = system_message2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d965eb61-6056-4465-b0f6-13eb0704d802",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_brochure(company_name, url, model):\n",
    "    prompt = f\"Please generate a company brochure for {company_name}. Here is their landing page:\\n\"\n",
    "    prompt += Website(url).get_contents()\n",
    "    if model==\"GPT\":\n",
    "        result = stream_gpt(prompt)\n",
    "    elif model==\"Gemini\":\n",
    "        result = stream_gemini(prompt)\n",
    "    elif model == \"Groq\":\n",
    "        result = stream_groq(prompt)\n",
    "    elif model == \"Ollama\":\n",
    "        result = stream_ollama(prompt)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown model\")\n",
    "    yield from result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "46e89c05-5293-46a4-a6ba-36b77c8c24a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7893\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7893/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "view = gr.Interface(\n",
    "    fn=stream_brochure,\n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"Company name:\", lines = 2),\n",
    "        gr.Textbox(label=\"Landing page URL including http:// or https://\", lines = 2),\n",
    "        gr.Dropdown([\"GPT\", \"Gemini\", \"Groq\", \"Ollama\"], label=\"Select model\")],\n",
    "    outputs=[gr.Markdown(label=\"Brochure:\")],\n",
    "    flagging_mode=\"never\",\n",
    "    js=force_dark_mode\n",
    ")\n",
    "view.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
